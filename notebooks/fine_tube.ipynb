{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amharic Telegram NER Fine-Tuning with LoRA and Hugging Face\n",
    "\n",
    "# STEP 1: Install Required Libraries (for Google Colab or local GPU env)\n",
    "!pip install transformers datasets accelerate peft seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eec9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Load Your CoNLL-Formatted Dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Manually Parse CoNLL-Formatted Dataset\n",
    "\n",
    "def load_conll_file(file_path):\n",
    "    examples = []\n",
    "    tokens = []\n",
    "    ner_tags = []\n",
    "\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    examples.append({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
    "                    tokens, ner_tags = [], []\n",
    "            else:\n",
    "                splits = line.split()\n",
    "                if len(splits) == 2:\n",
    "                    token, tag = splits\n",
    "                    tokens.append(token)\n",
    "                    ner_tags.append(tag)\n",
    "\n",
    "    if tokens:\n",
    "        examples.append({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bbfaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_data = load_conll_file(\"/content/ner_labeled_conll.conll\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_data)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ec7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['train'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374807db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Tokenization + Label Alignment\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Align token labels with tokenizer's subword split\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    word_ids = tokenized.word_ids()\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(label2id[example[\"ner_tags\"][word_idx]])\n",
    "        else:\n",
    "            labels.append(label2id[example[\"ner_tags\"][word_idx]])\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Load Model + Apply PEFT (LoRA)\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af126cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Trainer Setup\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "label_list = list(label2id.keys())\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./amharic-ner-checkpoints\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c1683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Train the Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098bbf12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbbe2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
